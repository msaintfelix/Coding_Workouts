{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series(num_timeseries, n_steps):\n",
    "    \n",
    "    freq1, freq2, phase1, phase2 = np.random.rand(4, num_timeseries, 1)\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    series = 0.5 * np.sin((time-phase1) * (freq1 * 10 + 10)) # First sine wave\n",
    "    series += 0.2 * np.sin((time-phase2) * (freq2 * 20 + 20)) # Second sine wave\n",
    "    series += 0.1 * (np.random.rand(num_timeseries, n_steps) - 0.5) # Noise\n",
    "\n",
    "    return series[..., np.newaxis].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate 10000 time series of 50 steps in order to predict one step ahead.\n",
    "n_steps = 50\n",
    "series = generate_time_series(10000, n_steps + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train contains 7000 time series of shape [50, 1], X_train is [7000, 50, 1]\n",
    "# y_train takes the last element of each series as a label\n",
    "X_train, y_train = series[:7000, :n_steps ], series[:7000, -1]\n",
    "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 50, 1)\n",
      "(7000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn (SimpleRNN)       (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input_shape is [None, 1] because a RNN model can process sequences of any length, and this is univariate time series.\n",
    "# Also we will use the simplest architecture: a single layer with 1 neuron.\n",
    "simple_rnn = keras.Sequential([\n",
    "    layers.SimpleRNN(1, input_shape=[None, 1])\n",
    "])\n",
    "simple_rnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each neuron, there is one parameter per input and per hidden step dimension (in this case it is the number of neurons in the layer), plus a bias term. That's just 3 parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "simple_rnn.compile(loss=\"mse\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the SimpleRNN uses a tanh activation function. The initial state is set to 0 and it is passed to a single recurrent neuron along with the value of the first time step x(0). The neuron computes a weighted sum of these values, applies tanh to the result and gives the result to the new state that is passed again to the same recurrent neuron along with x(1) for processing repeatedly until last time step until the layer just outputs the last value y49. This is done simultaneously for every time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "219/219 [==============================] - 3s 10ms/step - loss: 0.0187 - val_loss: 0.0110\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0114 - val_loss: 0.0110\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0112 - val_loss: 0.0111\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0112 - val_loss: 0.0111\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0113 - val_loss: 0.0110\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0113 - val_loss: 0.0112\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0113 - val_loss: 0.0111\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0110 - val_loss: 0.0111\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0113 - val_loss: 0.0111\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0113 - val_loss: 0.0111\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 1s 7ms/step - loss: 0.0110 - val_loss: 0.0110\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.0112 - val_loss: 0.0111\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.0113 - val_loss: 0.0110\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0115 - val_loss: 0.0110\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 1s 7ms/step - loss: 0.0109 - val_loss: 0.0112\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0116 - val_loss: 0.0110\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0113 - val_loss: 0.0111\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 1s 7ms/step - loss: 0.0113 - val_loss: 0.0111\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0113 - val_loss: 0.0111\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 1s 7ms/step - loss: 0.0117 - val_loss: 0.0111\n"
     ]
    }
   ],
   "source": [
    "# Our model will train on batches of 32 x 219x50x1. \n",
    "history = simple_rnn.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_rnn = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_rnn.compile(loss=\"mse\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "219/219 [==============================] - 6s 20ms/step - loss: 0.1971 - val_loss: 0.1761\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 3s 16ms/step - loss: 0.1432 - val_loss: 0.1753\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.1488 - val_loss: 0.1674\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.1479 - val_loss: 0.1537\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.1486 - val_loss: 0.1496\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 4s 16ms/step - loss: 0.1519 - val_loss: 0.1475\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.1470 - val_loss: 0.1581\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 5s 24ms/step - loss: 0.1470 - val_loss: 0.1473\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.1421 - val_loss: 0.1487\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 3s 14ms/step - loss: 0.1437 - val_loss: 0.1473\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.1433 - val_loss: 0.1484\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 4s 19ms/step - loss: 0.1455 - val_loss: 0.1615\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 4s 17ms/step - loss: 0.1412 - val_loss: 0.1473\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.1454 - val_loss: 0.1476\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.1446 - val_loss: 0.1479\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 3s 14ms/step - loss: 0.1460 - val_loss: 0.1524\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.1499 - val_loss: 0.1508\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.1464 - val_loss: 0.1481\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 4s 16ms/step - loss: 0.1447 - val_loss: 0.1778\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.1452 - val_loss: 0.1479\n"
     ]
    }
   ],
   "source": [
    "history = deep_rnn.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5164187 ,  0.40241325,  0.23317498, -0.03226947, -0.2284828 ,\n",
       "       -0.35232672, -0.35885486, -0.27879146, -0.265251  , -0.3167747 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series[7000, -10:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
